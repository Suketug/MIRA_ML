# -*- coding: utf-8 -*-
"""train.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ou8-HNGWgKQhgHg0O3Cnxwa16Mgm7OO7
"""

# Import required libraries
from transformers import T5ForConditionalGeneration, T5Tokenizer, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer
from torch.utils.data import Dataset
import pandas as pd

# Custom Dataset
class CustomTextDataset(Dataset):
    def __init__(self, tokenizer, data_file, text_col, target_col, max_length=512):
        self.tokenizer = tokenizer
        self.data = pd.read_csv(data_file)
        self.text_col = text_col
        self.target_col = target_col
        self.max_length = max_length
        print(f"DataFrame Length: {len(self.data)}")  # Debug statement 1

    def __len__(self):
        return len(self.data)

    def __getitem__(self, index):
      text = self.data.loc[index, self.text_col]
      target = self.data.loc[index, self.target_col]

      # Tokenize both the text and target
      inputs = self.tokenizer(
          text,
          add_special_tokens=True,
          max_length=self.max_length,
          truncation=True,
          padding="max_length",
          return_attention_mask=True,
          return_tensors='pt'
      )

      targets = self.tokenizer(
          target,
          add_special_tokens=True,
          max_length=self.max_length,
          truncation=True,
          padding="max_length",
          return_attention_mask=True,
          return_tensors='pt'
      )

      # Squeeze unnecessary dimensions
      inputs = {key: tensor.squeeze(0) for key, tensor in inputs.items()}
      targets = {key: tensor.squeeze(0) for key, tensor in targets.items()}

      return {
          'input_ids': inputs['input_ids'],
          'attention_mask': inputs['attention_mask'],
          'labels': targets['input_ids'],
      }

# Initialize the T5 base model and tokenizer
model = T5ForConditionalGeneration.from_pretrained("t5-base")
tokenizer = T5Tokenizer.from_pretrained("t5-base")

# Initialize custom dataset
train_dataset = CustomTextDataset(tokenizer, "/content/train_summarization.csv", "Cleaned_Description", "summarized_text")
val_dataset = CustomTextDataset(tokenizer, "/content/val_summarization.csv", "Cleaned_Description", "summarized_text")
test_dataset = CustomTextDataset(tokenizer, "/content/test_summarization.csv", "Cleaned_Description", "summarized_text")

# Data Collator with Debugging
class CustomDataCollatorForSeq2Seq(DataCollatorForSeq2Seq):
    def __call__(self, batch):
        # Call the original data collator
        batch = super().__call__(batch)
        return batch

# Data Collator
data_collator = DataCollatorForSeq2Seq(
    tokenizer=tokenizer,
    model=model
)

# Training Arguments
training_args = Seq2SeqTrainingArguments(
    output_dir="./results",
    overwrite_output_dir=True,
    num_train_epochs=3,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=32,
    warmup_steps=500,
    weight_decay=0.01,
    logging_dir='./logs',
)

# Initialize Trainer with CustomDataCollator
trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    data_collator=CustomDataCollatorForSeq2Seq(tokenizer, model),
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
)

# Train the model
trainer.train()

# Save the trained model
model.save_pretrained("/content/Trained_Models/summarization_model")

# Evaluate on test data
test_results = trainer.evaluate(test_dataset=test_dataset)
print("Test Results:", test_results)