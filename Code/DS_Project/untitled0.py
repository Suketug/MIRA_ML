# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ou8-HNGWgKQhgHg0O3Cnxwa16Mgm7OO7
"""

!pip install transformers
!pip install torch torchvision
!pip install pandas
!pip install sentencepiece
!pip install transformers[torch]

# Import required libraries
from transformers import T5ForConditionalGeneration, T5Tokenizer, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer
from torch.utils.data import Dataset
import pandas as pd

# Custom Dataset
class CustomTextDataset(Dataset):
    def __init__(self, tokenizer, data_file, text_col, target_col, max_length=512):
        self.tokenizer = tokenizer
        self.data = pd.read_csv(data_file)
        self.text_col = text_col
        self.target_col = target_col
        self.max_length = max_length
        print(f"DataFrame Length: {len(self.data)}")  # Debug statement 1

    def __len__(self):
        return len(self.data)

    def __getitem__(self, index):
        print(f"Fetching data for index: {index}")  # Debug statement 2
        text = self.data.loc[index, self.text_col]
        target = self.data.loc[index, self.target_col]
        inputs = self.tokenizer(
            text,
            target,
            add_special_tokens=True,
            max_length=self.max_length,
            truncation="only_first",
            padding="max_length",
            return_attention_mask=True,
            return_tensors='pt'
        )
        print(f"Input shape {inputs['input_ids'].shape}")  # Debug statement 3
        return inputs

# Initialize the T5 base model and tokenizer
model = T5ForConditionalGeneration.from_pretrained("t5-small")
tokenizer = T5Tokenizer.from_pretrained("t5-small")

# Initialize custom dataset
train_dataset = CustomTextDataset(tokenizer, "/content/train_summarization.csv", "Cleaned_Description", "summarized_text")
val_dataset = CustomTextDataset(tokenizer, "/content/val_summarization.csv", "Cleaned_Description", "summarized_text")
test_dataset = CustomTextDataset(tokenizer, "/content/test_summarization.csv", "Cleaned_Description", "summarized_text")

# Data Collator with Debugging
class CustomDataCollatorForSeq2Seq(DataCollatorForSeq2Seq):
    def __call__(self, batch):
        # Call the original data collator
        batch = super().__call__(batch)

        # Debugging
        if 'input_ids' in batch:
            print("Debug: input_ids shape", batch['input_ids'].shape)
        if 'attention_mask' in batch:
            print("Debug: attention_mask shape", batch['attention_mask'].shape)
        if 'labels' in batch:
            print("Debug: labels shape", batch['labels'].shape)

        return batch

# Data Collator
data_collator = DataCollatorForSeq2Seq(
    tokenizer=tokenizer,
    model=model
)

# Training Arguments
training_args = Seq2SeqTrainingArguments(
    output_dir="./results",
    overwrite_output_dir=True,
    num_train_epochs=3,
    per_device_train_batch_size=32,
    per_device_eval_batch_size=64,
    gradient_accumulation_steps=2,
    warmup_steps=500,
    weight_decay=0.01,
    logging_dir='./logs',
)

# Initialize Trainer
trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    data_collator=data_collator,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
)

# Debugging snippet
for batch in train_dataset:
    input_ids = batch['input_ids']
    print(f"Debug: Input shape {input_ids.shape}")

# Train the model
trainer.train()